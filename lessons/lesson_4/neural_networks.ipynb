{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652354d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install\n",
    "try:\n",
    "  import imlms\n",
    "  print('Already installed')\n",
    "except:\n",
    "  %pip install git+https://github.com/Mads-PeterVC/imlms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87468ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(install.stdout.splitlines()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4073017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1d16ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Linear Regression with a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1687de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.torch import get_example_data\n",
    "\n",
    "X, y = get_example_data(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab481e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1) # One input and one output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "y_pred_init = model(X).detach().numpy()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: loss {loss.item()}')\n",
    "\n",
    "y_pred_final = model(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706358fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, y_pred_init, label='Initial prediction')\n",
    "ax.plot(X, y_pred_final, label='Final prediction')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "638e0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: More flexible architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "100404fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the cell below finish defining the model by completing the `forward`-function.\n",
    "\n",
    "To do this, you should \n",
    "\n",
    "1. Call the first linear layer `linear_1` with the input `x` and assign that to `y`\n",
    "2. Apply the activation function `torch.relu` to `y` and assign that to `y`\n",
    "\n",
    "And then proceed to apply the second linear layer, an activation function and then the third linear layer \n",
    "and return the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonlinearRegression, self).__init__()\n",
    "        self.linear_1 = torch.nn.Linear(1, 16) # One input and 16 outputs\n",
    "        self.linear_2 = torch.nn.Linear(16, 16) # 16 inputs and one output\n",
    "        self.linear_3 = torch.nn.Linear(16, 1) # 16 inputs and one output\n",
    "\n",
    "    def forward(self, x):            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "6ed8ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finish the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9509c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NonlinearRegression()\n",
    "y_pred_init = model(X).detach().numpy()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    pass # Your code here\n",
    "\n",
    "y_pred_final = model(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8a2f5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "You will see that the fit you get will be a little jagged. \n",
    "\n",
    "The model has gotten more expressive - but we always have to think whether it is a \n",
    "better model in general or if its a too complex model that is **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, y_pred_init, label='Initial prediction')\n",
    "ax.plot(X, y_pred_final, label='Final prediction')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "55b34062",
   "metadata": {},
   "outputs": [],
   "source": [
    "Try the following and see how it affects the fit\n",
    "\n",
    "- Train the model for more steps and maybe with a lower learning rate\n",
    "- Increase the number of neurons in each layer.\n",
    "- Add an additional linear layer followed by an activation function to the network "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2df62814",
   "metadata": {},
   "outputs": [],
   "source": [
    "Because of the enourmous flexibility of neural networks to fit anything being vary of \n",
    "overfitting is important. \n",
    "\n",
    "Therefore it is generally not the performance on training data we use to judge whether a model works but \n",
    "the performance on a test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf236e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_example_data(n=25)\n",
    "y_pred_test = model(X_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, y_pred_init, label='Initial prediction')\n",
    "ax.plot(X, y_pred_final, label='Final prediction')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.scatter(X_test, y_test)\n",
    "ax.plot(X_test, y_pred_test, label='Test prediction')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "862f3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2539acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "The cell below defines another dataset (That you have seen before..)\n",
    "\n",
    "Your task is to setup and train a neural network that can fit this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b70b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.torch import get_damped_osc_dataset\n",
    "\n",
    "X, y = get_damped_osc_dataset(n=10)\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_test, y_test = get_damped_osc_dataset(n=50)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No help this time. You can do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "y_pred_final = model(X).detach().numpy()\n",
    "y_pred_test = model(X_test).detach().numpy()\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, y_pred_final, label='Final prediction')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.scatter(X_test, y_test)\n",
    "ax.plot(X_test, y_pred_test, label='Test prediction')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "45aac4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Neural Network Pair Potential"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8a1bd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets try fitting a pair potential with a neural network, well stary by getting some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.potentials.load_cluster_data import get_cluster_data\n",
    "\n",
    "X, _, E = get_cluster_data(train=True)\n",
    "X_test, _, E_test = get_cluster_data(train=False)\n",
    "\n",
    "print(X.shape) # (n_samples, n_atoms, cartesian_coordinates)\n",
    "print(E.shape) # (n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "56e1c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "The data consists of 300 two-dimensional configurations each with 12 atoms and the corresponding total energies.\n",
    "\n",
    "A pair potential is a potential where only the bond-length between pairs of atoms matters, like the \n",
    "Lennard Jones potential we worked with previously. So our neural network will be a function like \n",
    "\n",
    "$$\n",
    "\\mathrm{NN}_\\theta(r) \\rightarrow R^1\n",
    "$$\n",
    "Such that we can feed the network with all the bond distances and it predicts the energy for each bond. \n",
    "Finally, we can sum all these contributions together to get the total energy of a configuration. \n",
    "\n",
    "$$\n",
    "E_{tot} = \\sum_{r \\in R} \\mathrm{NN}_\\theta(r)\n",
    "$$\n",
    "Where $R$ is understood to be the set of all distances in a configuration of atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PairNN, self).__init__()\n",
    "        self.sequential_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 16), # 1 input, 16 output\n",
    "            torch.nn.SiLU(), # Look at torch documentation if you want to know what this is.\n",
    "            torch.nn.Linear(16, 16), # 16 input, 16 output\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(16, 1) # 16 input, 1 output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = torch.pdist(x) # Get the pairwise distances\n",
    "\n",
    "        # Put each distance through the model\n",
    "        e = self.sequential_stack(r[:, None])\n",
    "        return e.sum() # Sum over all pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4f766408",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can define a nice little training loop function to train this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, X_train, y_train, epochs=200, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    loss_per_epoch = torch.zeros(epochs)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)): # Loop over the dataset multiple times\n",
    "        optimizer.zero_grad()   # Zero the gradients\n",
    "        loss = 0                # Initialize the loss\n",
    "        for xb, yb in zip(X_train, y_train):# Loop over the dataset\n",
    "            E = model.forward(xb) # Compute the energy\n",
    "            loss += loss_fn(E, yb)\n",
    "        loss.backward()         # Compute the gradient\n",
    "        optimizer.step()        # Update the parameters\n",
    "        loss_per_epoch[epoch] = loss.item() / len(X_train) # Store the loss\n",
    "        \n",
    "    return loss_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a6db31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "And finally we can try training the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PairNN()\n",
    "loss = training_loop(model, X, E, epochs=10000, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "86bd05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Its a good idea to see how the loss has progressed as a function of epochs / gradient steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b780b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot(loss)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4e58f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "And finally we can create a parity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28915062",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), layout='constrained')\n",
    "\n",
    "E_pred = torch.zeros_like(E)\n",
    "for i, x in enumerate(X):\n",
    "    E_pred[i] = model.forward(x)\n",
    "\n",
    "E_test_pred = torch.zeros_like(E_test)\n",
    "for i, x in enumerate(X_test):\n",
    "    E_test_pred[i] = model.forward(x)\n",
    "\n",
    "for ax, (E_true, E_model) in zip(axes, [(E, E_pred), (E_test, E_test_pred)]):\n",
    "    ax.scatter(E_true, E_model.detach().numpy())\n",
    "    xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "    lim = (min(xlim[0], ylim[0]), max(xlim[1], ylim[1]))\n",
    "    ax.plot(lim, lim, 'k--')\n",
    "    ax.set_xlim(lim); ax.set_ylim(lim)\n",
    "\n",
    "    ax.set_xlabel('True energy [eV]')\n",
    "    ax.set_ylabel('Predicted energy [eV]')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "82a09e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Neural Network Pair Potential "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "08294f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "The data in the previous exercise comes a pair potential, so it can be modelled perfectly with \n",
    "a pair potential. \n",
    "\n",
    "However, now we will try a dataset where the energies have been calculated with DFT, \n",
    "as such it is very much not from a pair potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.potentials.load_carbon_data import get_carbon_cluster_data\n",
    "X, E = get_carbon_cluster_data(n=6) # n is the number of atoms in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "051aa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Try fitting this dataset, you can play around with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8041960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "# Train the model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot(loss)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "E_pred = torch.zeros_like(E)\n",
    "for i, x in enumerate(X):\n",
    "    E_pred[i] = model.forward(x)\n",
    "\n",
    "ax.scatter(E, E_pred.detach().numpy())\n",
    "xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "lim = (min(xlim[0], ylim[0]), max(xlim[1], ylim[1]))\n",
    "ax.plot(lim, lim, 'k--')\n",
    "ax.set_xlim(lim); ax.set_ylim(lim)\n",
    "\n",
    "ax.set_xlabel('True energy [eV]')\n",
    "ax.set_ylabel('Predicted energy [eV]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "81fbe9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Classification with a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4036a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "35eda822",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification is another type of supervised learning - one can think of it as discrete regression. \n",
    "\n",
    "The cell below generates a classification dataset and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9903788",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "X_np, y_np = make_classification(n_samples=100, n_features=2, \n",
    "                           n_classes=3, n_redundant=0, \n",
    "                           n_clusters_per_class=1, random_state=41)\n",
    "ax.scatter(X_np[:, 0], X_np[:, 1], c=y_np, cmap='viridis')\n",
    "\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_xlabel('Feature 1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d3f5fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we will construct our neural network model for this classification task. \n",
    "\n",
    "In this case our network will be rather simple consisting of only two linear layers\n",
    "\n",
    "1. The first layer should take the two dimensional input features and convert them to 16 dimensions. \n",
    "\n",
    "2. The second layer should convert the 16 dimensional representation to three dimensions - one for each of the classes. \n",
    "\n",
    "In between the two layers we will use a ReLU (`torch.relu`) activation function.\n",
    "\n",
    "In order to make discrete predictions we will use **softmax**, illustrated below"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0a5582e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "![softmax](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAACvCAMAAABqzPMLAAAAkFBMVEX///8AAAD6+vrr6+tQUFDW1tYRERHe3t6AgIDh4eGgoKBmZmampqZERET29fZUU1TFxcXX19fLy8vu7u5LS0uSkpKampq/v7+pqamUlJSMjIxzc3NZWVmEhITPz89ubm60tLReXl4uLi4zMzNpaWklJSUbGxs6OjoQEBA+Pj5GRkYYGBghISEpKimxsbK6ubprhkMNAAARf0lEQVR4nO2dCXuqOhCGZ4IWEI0sCsiOS931//+7Owmitj2tQO3m5XvO06MgIXnJOkkGgFatWrVq1apVq1atWrVq1apVq1atWrX6hNhkOU3V81ebvzxr3uMefDzbh0r5rTvr8O49gv0ejTaJOsHk9E1ZvQRkz+5xj7iTWOGyDHkec829R7DfomzL6G/QAc8C6DsTdB3fDA8AvgK872jbwx1usjxSLqIspAdaAuY+zAZDy+sGmmKnGYDuhj7jrgNW/w73urcMTfzVkRs+wErto6+j4U1dmCbgoBI8e3e4iYuaKKoKjn30ujPNG+V6iL3wyfDRZjg5rmPIQ4b3eBj31kg+NQX1OcVuoSpbpiAHC9mSAK0ce32Xu9gaYgBxCHBcQORB2INUE48Fwj73wJmMgM+m8V3udWeFofjbRS4A7VR9y/UdiLyzloDMewBiooazMcn74k4SkAupC/pKAIJ4Nhp1AMao3+Fed1cio2XMYe4TljIHUTKWNqjomMs73IPLNmDgBS9yUK8A5B8XDvQ3oC7CUeM7OMNoGAXO1ZH+dXFNZQvq99mcZy4oKmgJVFaMrj9cKOAuspAAoatjKuqgdJQN0eli1jjWFwUY9/MdV7CXUT3TOUIwsCgHqfQo0r6H5uFpDWuq9iZNb6Bj5mXRNWBtfPXlSeZN0+To2EfYHMGvk1kTLfVFS9YLbd+BY6yuMtG2cDfs9jn0x7euryI7nPeoldfj0AY4qOAEnkdF2GXgdeEwd5WJTp/1Mb8d1D+lI5P1gpfECRy0AwMt7ge6aCE1SsvCC+j23cRBx0qsbap4Ct1co7joQVy7p6djw1j+oER1zzJkEYZ2OMzynArGxEWFCsRx78JsejAiiDWO9Nj1XaiMPLYIst2R7SlP2zVv5uRfkoYvlYLr9ZRKbxTQRwfYVhUtZjjhJijaHHYmsBVlFgmIKkPomIe1ovhTB31Fd26Hz9hXxPptqP+6j/LvYlUvSjraXZUSOjxAd09Xdsy4R90vDbTFaDSHZzq1TEpA6yN07Ml2M9gMwRxhVKE+WtfNZRWi7IP/MivaJkT+m9/Nsff2YutIT7/W3bDg2TmAaITZsxVTpyp0/QGHiQG7LmUq/RqQ2R9SWTk6HvB5ejv8qaio7piLKCjfEM3p1QHIfVCV17/k+OYQiJSBXqtTJOogoRE9gXluz0egod9DflgkGUawWJppLusgx03ByKkO4qvAnsYOusmgwrhwasMh32hOQuMOPYUkH40ZiyfLmmNuHndEDL3Opuesn3t2LwkoPA2yfBM6yXbZdU0RdsAh7kUj2XrkmNv0BO0Agl7UoUP+pnO09jPPz8CaU5Rg0o+oX3DrzuPi8foUY9afuxw8L9B0+my4qst6dkiHTI9NGDXzupZQQ6oE1PMDNRYtdgVAFnWBooBTr3EcOitPMQKYDo8Vqq9raamSoKqiqT8f3Yj7OTWrQENSTBQjhmgMQxroHXUjhFmHRn3ikmSVUKcUsg0MRqqP7Ihqgt1UY+GYUwUaRhDN1AxrRuTuohzkqMdhCIYLe9WfeV5vy/e1OwjcSfqYjENR81IRozqIRsB7S4RtGOJzdDjksh8xM0+Fy3niqgDUgbUnDlEEQAfNpTrIHFBuQCXqA3/+6UHI1FZ204CerD1LZjCZaqE25gurbjA+Dnvbbiy7lgUgc5aswXnea2FeAHIFPeQzW6QdBCCnBGQLQJEcIUhAh4g+oR5lwBc/Diihqh7SOTWGnR54NP6yAlYfEJrUm7UPGxpb9DIJCBZDH3o0BNCGMO8TIHNBnecdXACtHFHBxsMCkBNThZTb8YQAWU8OWMgFoN1PA9rZJo7DQSQsN1Sz5ZsYfVjVBjQfTSLM2CgPF4qNgcgCY8JAH8PBlAZkJjXzxkbbZrLnVgCi6mUwMJbDU6lTFqExYj4ewgDCdbzqw4gArX4aUMLBOlichriZyNdgUy0vDtYU844ONc/MFLV7kohmXoRJYSf0PzN1SxFhU2oT+oEtGh6WMOBH27Gg65y+eIx+aolmPsnoGXVFWF/SkW0g1tvdxUj/sGLhbzR6tmrVqlWrVq1afYGYaSZJN3mnJ98lJeYfWohRSCSq+26iLJmoqvNiCkr92+jIipOVpvN4oH2H1NsxEeM7qX/PPfCVPLmvEhBJeboNaFglIB2/AY8Yu1bQx4CKk1UnjAmQ2U3ey42UUa0YjSoBfc9k2rASIGJweL+IUQmzgmrFAiSgG9bMXlVA3zHgrgzohqXGx07FOxKg86yCc14JpPreObl/E5BsWI5xr7TYqEEgqmV2iHsiY/WbAGLDzemTi9FiUwb9uwD9Yw7trU6AtNV4OShilWAYog28s5hEInc1AWRtMCo+OXik+rmMyu8CVCMHqfSHYyaPjDSAeA1HkdgobQSIb7XgVHFxUdSmwekXfxZQf0ofQ9mUOWJ9QoJcFSnT8mY5yIHxpWY3w21Z8v4sIPm8xwNxwBKJ1IuKSUe/YR10BYgPZ+uyS/ZnAYWixnDlCsKuaKoVucxN2YnkfBYQySjXJv5ZQLFITiAbnlMOotrZWs1FPD8HyBGkzbJ/9GcByTpITPVd6iDRmslffA6QjVS84nKw8mcB6UTFwQMw6vh0KA3hmkrZqelpBigQpSpecMifeyGW0z9/FhAEmG6pxfLoeVs4GtKxEFfF2LIZoCSjP7bPqL8ZBudh898FBHZPDAgUj/q8Sl+sa0080xOLRxsC+rf+MKD31QL63wKqMxb7QI8LqM1BH6sFdEMtoBtqAd1QC+iG/m+AqHN74+6v9JOAemmaGuUAg102eHwdILbBoVdvZ1GtfpA+xOlpRMk0XInFyXpUHGoAiE07YZiesPAcv2EsZvf0Pg2NYXxzo8FZdXIQm6bq5LQNdr61k5kGfDa3xDLeJoCUq6KWLNb4DTmILpLeGCbVJ//rAJIWraVc1K4I24SNTB7aBI0A2ZgcSguH39e/IweBJffa1rmuDiBpbY0lCEuA0dFyhYVaixoB6uNiiJ0yssrX5iDdFuZKdc5AdS2j0tx/oTqANBHuZF0c8MVsahLLQ7NGgLyxSGRZBypfmYOO01lOd1Kmw2iOetBdVUdbC5Aw2heAKBFpOsVEHnKbAZIyShZfCUgTDii8LTtYEMy7wMZaxQuhHiA54xOcQJiaa6E+ESb8uJlF0c3oTxSWh78OkCvtwkE5zQ18362+yb8OoKPYhLiR9FlPEaZX+ie2tWqNAAX0v4k2WHIFw9cB4gt0D5PhZSdRd3d4u5H1PdUBxDFgR/qgThxYGkx/DsDBWB5q1A8ycIsuFYCtWLXwdYBMfA7G/as8w3rV+dTrSSc7xEzcUQdljSjyUrJH6WGmUR2k2nJ7UtF86V+1/OWA4e0fvauaFsWr6rXcfF622b92LOZhsd1bbTS2/R8MVp2tDM5dN/LS8T8ABBnGR3c6b+bF5P8ACLoB9Ukq/va1/heAPqMW0A21gG6oBXRDLaAb+lFAjnVuepl6Wcb/hwHp3pkTM02ROqdrWV2lGSANsTQHmVvETrlD4SYgJxZofgcgU6bgBMjHWTmuURbb560uhuQoHF42syiqNKwzi/Bd4OtyuuEmoC6O2AtAyd11HjnfAuTi+AxIQU8uwxMyIoCcUtQJGOesmblDDJHWcpzUFev3syd2AaR035d1QEHoAui03+ieiq4AqVRK3pNKxWBcAjosKEaF+y1eGO05f7IL12gNK2kdL45tjTIEAci4lYIOv8pB6t11juKwz27SHJcLyYVNrjDMqcLErqOq4xCf3caA2GhzTmT/7GBaAMpvxkr7rjroNiD02DsLybs2uvpB1LSNAPHl8rx1zMXzZJ4AFN2K01b9PYAiflpILhi82IpgySmmYNoMkP7cOTfzMV58wAhAZjB+VxMq99RAfFsr1ns/KuMJPcmcnYpYT/hS12RbI3chUDZyRFp9immTmdXdxdKn7a72zt5sxSx8Vn9LM+9ixMpKOhFm4xWVJ3ruyxhgsqXTVEaMZrt9Qhy7k7EJPYMlaLiTiXvVin0oXe57qQSIDaOo0weeRp0ahugr3TK5HnJRCk79oHzaz/cc7JUORwwmVGs4u5GfNttQx9I8Gopo91Pw8zyKIqMqoGLuuFoOUjo7lVNroFkNDWZ1bNJ8YsSK8JRI2cacp6KFVsaG3Fn+a8di7El4sswbO0Z9+GXAwn23bdRbNHWtRwGk0PDAFjJN+6TirItq7zNeNR8FUB9xMDeEqDobyN5JUaqGizDHT7yH4lEAUc9yffVVn2yLYTXDqcUuI6sPdBhz/o+G7mEAKauXE6iOId0ZyzcIxLfcIoibTkcD2XtgL1u7hwEEHuKLDMDW4qtcH6jg7fUuCYNQlsT05SsHHgeQsL+9yChJClwfbFUGyh7Nm70gphU1lfKyRn8gQHyJr98rdYxyI9dpeGQMby3bZKENnCh6r5YOPRAgsFYVitJr8SycZ8Sn02fOSIXAw5celX4SEPeDk8sX3ScdslMxaNxRpLa++gLoQt4uyrxtH7RxiDgB5chePb56Qw030srJB9PIizLryFUBTRZQdZba6ZGbi9lsjZ/fN59iTUfiR9n0xSlQwbJlDWa+8qRVC5CxzPJd8Zg9dAsTl9OR6W0A6CDfeHROkI7Z6VNzQHwrjQ+V5WxFX8C8skXJ921dqw4g+Vanp8I+ItaTuzsCtdpvxYNvACilqLDZ+f6bM5VPjMUSrNVrFv3v6XZw7a959ur6OoAKg5nslUqDmTArhr6NDQENxNrTTlmtXr2G4DOD1ejtpW8a+Es7PsdAV65CYVzBVz+vA+ityVWUO7MhILYUxv78lKPZ+tIAfQJQb/AGx6tmSSnWdhcy8KWz794oeL2esQ4g6TrojfeXpoBOOej0Dj3rKhnNAZmrtz2I5OULsg7x4BLToJhJscp31lia/zrMOoCk/6DCaVQ57QOfABSmYgPRyeQ7Xl5i1hiQVWHMRfe9+ORRRCPGD/iB0/g6gPwdK8sEF3WhLWPYGNCRLrQJtDT8D6+6eE0BKYsXvSAu7B1O/MaVZnrltMhcrEezD81pdQApeKCs44FDAxtjBCyXhbkxIBY9p2JaX3viwK4Hmg0BsdHLVwr15nIfoxGCe3asKToV14CAq+rHo7Ra/SAf16JnJ6o9ZbFbFC8vaQwI2LEnHrIqMrh39RQbApq/rF+7wmLmHWEUgG6VM8oCRlrrxY/1hhq6KQo5F0uyuX16RTOXWfjnx2KTlxHwnrZF9N50jL4S0Pv6cUAZqkohXbUP8R6xaB89utskLCXyfL1Xhz4KoO7bGfPCbqENGCh6KXHF/zMHKaepDNM0y4mN4uzi1cuKaTS5iiolutCjAHpP/HUfhyuOo9SYBnpoQHx57K+azTif9dCAFIyWlTyuf6CHBgRKwxULV3psQHdQC+iGagJyLnHip9zrfGKV6+FykbP7pY4m6wBKnhFP5glmiLWLwrCMqDVbJy16v+eL9PW3+O6orzqA+GrCjyeTSzzV9aUGFvpOd+U2G82nOCwv8vBb3OM0UB1AnuhTFCZA9pQJgw4chcWjoUdy7jpu2es/2l/qu+MTqgNIuu0oLIrSmCjMiiKOm/AODrcfAdCVc5MrmzSMGzpYgscDJH3hyFmNcqU9iMU3wurVAjo7WJJ2VlnExNwP1bSyA9ACOrvoKlyQ7/qiiQeY7z/jmmIsdsaYPZHQRwDEpoZ08tbNFXDRtHFCjLS+KxwBNQPkipm2QC4uVJ6/xclbbdVzExg9r4Wz/yWlMFjsA9GmjTabdeNWTK4HPC0KvAwr/y6gD/zH1QL0p16f9ROD1YcEdM/3iz09Pb2zWI6tttvtqtK65l/1hjoHcbt9eseTE5+tnihRlV/hV8w5fPiOw0qwdfTMr9esUg6q8hLIaZWAxM/HgdA7iwnpDJ2v9tSiwddrM6q0T4jJDZLBv98Mz1w6GwTuP0+2atWqVatWrVq1atWqVatWrVq1atWqVatWrVq1ehD9B1gPV90gIDiMAAAAAElFTkSuQmCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d837fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Softmax converts a set of numbers to probabilities proportional to the size of the numbers, e.g. \n",
    "in the figure 5.1 is the largest number and is converted to the largest probability. (If you have had statistical physics / thermodynamics you might see a similarity to e.g. the Boltzmann distribution - \n",
    "where the denominator is the partition function.)\n",
    "\n",
    "With this we can have our network predict the probability of an example being of either of the classes. \n",
    "In order to make a final prediction we would then take the class with the highest probability\n",
    "\n",
    "In torch we can use `torch.softmax` to apply softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_features=2, n_classes=3, n_hidden=16):\n",
    "        super(Model, self).__init__()\n",
    "        # Your code here: Make the two linear layers.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Make the forward pass.\n",
    "        pass # Replace with your code \n",
    "            \n",
    "    def predict_probabilities(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return torch.softmax(logits, dim=1)\n",
    "\n",
    "    def predict_label(self, x):\n",
    "        probs = self.predict_probabilities(x)\n",
    "        return torch.argmax(probs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "20f4874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "To test our model before training it, use the cell below to confirm the following\n",
    "\n",
    "- The forward pass of the model when given a (1x2)-vector produces a (1x3)-vector of numbers.\n",
    "- The `predict_probabilities` method produces a (1x3)-vector that sums to 1 and where the largest probability corresponds to the largest value produced by the forward pass. \n",
    "- The label predicted by `predict_label` is the index of the largest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ea058",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0, 2.0]])\n",
    "\n",
    "model = Model()\n",
    "\n",
    "logits = model(x).detach().numpy()\n",
    "print(f\"{logits = }\")\n",
    "probabilities = model.predict_probabilities(x).detach().numpy()\n",
    "print(f\"{probabilities = }\")\n",
    "print(f\"{probabilities.sum() = }\")\n",
    "\n",
    "label = model.predict_label(x).item()\n",
    "print(f\"{label = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9f74fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we will start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to tensors\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "73598eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training the model is no different than any of the other models we can have tried, \n",
    "except we will use a different loss function. Namely, we will use what is called\n",
    "cross-entropy. For two discrete probability distributions $p$ and $q$, cross entropy is given by\n",
    "\n",
    "$$\n",
    "H(p, q) = -\\mathrm{E}_p [\\log(q)] = - \\sum_{x \\in X} p(x) \\log[q(x)]\n",
    "$$\n",
    "\n",
    "Where $\\mathrm{E}_p$-denotes the expectation value under the distribution $p$.\n",
    "We can dwell on the [det](https://en.wikipedia.org/wiki/Cross-entropy)-[ails](https://stackoverflow.com/questions/41990250/what-is-cross-entropy) or we can just accept this for now.\n",
    "\n",
    "If we choose to accept it then using cross-entropy is easy as it is already available in \n",
    "pytorch using `torch.nn.CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ace586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss() # This is the loss function we will use. \n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 201\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad() # Zero the gradients\n",
    "    y_pred = model(X) # Forward pass\n",
    "    loss = criterion(y_pred, y) # Compute the loss\n",
    "    loss.backward() # Compute the gradients of the loss\n",
    "    optimizer.step() # Update the weights\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "160574d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can then make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict_label(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f6366416",
   "metadata": {},
   "outputs": [],
   "source": [
    "And compare with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615403bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "axes[0].scatter(X_np[:, 0], X_np[:, 1], c=y_np, cmap='viridis')\n",
    "axes[1].scatter(X_np[:, 0], X_np[:, 1], c=predicted_labels, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a4a5c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally we can plot the decision boundaries of our model, or the regions in which \n",
    "each label will be predicted by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a meshgrid\n",
    "x_min, x_max = X_np[:, 0].min() - 1, X_np[:, 0].max() + 1\n",
    "y_min, y_max = X_np[:, 1].min() - 1, X_np[:, 1].max() + 1\n",
    "\n",
    "dx = dy = 0.05\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, dx),\n",
    "                    np.arange(y_min, y_max, dy))\n",
    "\n",
    "# Turn into tensor\n",
    "X_mesh = torch.tensor(np.column_stack((xx.ravel(), yy.ravel())), dtype=torch.float32)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = model.predict_label(X_mesh).detach().numpy()\n",
    "\n",
    "predicted_probs = model.predict_probabilities(X_mesh).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(4*4, 4))\n",
    "\n",
    "axes[0].pcolormesh(xx, yy, predicted_labels.reshape(xx.shape), cmap='viridis')\n",
    "\n",
    "for i in range(3):\n",
    "    ax = axes[i+1]\n",
    "    c = ax.pcolormesh(xx, yy, predicted_probs[:, i].reshape(xx.shape), cmap='viridis', vmin=0.0, vmax=1.0)\n",
    "    ax.set_title(f'Probability of class {i}')\n",
    "\n",
    "plt.colorbar(c)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
