{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install\n",
    "try:\n",
    "  import imlms\n",
    "  print('Already installed')\n",
    "except:\n",
    "  %pip install git+https://github.com/Mads-PeterVC/imlms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(install.stdout.splitlines()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8a33cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imlms.potentials.load_carbon_data import get_carbon_cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "96ad021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convenience Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "62075db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "I am going to put a few functions in this section for convenience as they will be\n",
    "reused across the rest of the examples and exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Linear(vector_dim, hidden_dim)) # vector_dim -> hidden_dim\n",
    "        layers.append(torch.nn.SiLU())\n",
    "        layers.append(torch.nn.Linear(hidden_dim, hidden_dim)) # hidden_dim -> hidden_dim\n",
    "        layers.append(torch.nn.SiLU())\n",
    "        layers.append(torch.nn.Linear(hidden_dim, 1)) # hidden_dim -> 1\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddac915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, E, test_size=0.5):\n",
    "    n = len(X)\n",
    "    indices = np.random.permutation(n)\n",
    "    split = int(n * test_size)\n",
    "    train_indices, test_indices = indices[split:], indices[:split]\n",
    "    return X[train_indices], E[train_indices], X[test_indices], E[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7aa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, X_train, y_train, epochs=200, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    loss_per_epoch = torch.zeros(epochs)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)): # Loop over the dataset multiple times\n",
    "        optimizer.zero_grad()   # Zero the gradients\n",
    "        loss = 0                # Initialize the loss\n",
    "\n",
    "        # This commented part is equivalent to the below, but much slower.\n",
    "        # for xb, yb in zip(X_train, y_train):# Loop over the dataset\n",
    "        #     E = model.forward(xb).squeeze() # Compute the energy\n",
    "        #     loss += loss_fn(E, yb)\n",
    "\n",
    "        # Compute loss for the whole dataset as a batch\n",
    "        E = model.forward(X_train).squeeze() # Compute the energy\n",
    "        loss = loss_fn(E, y_train)\n",
    "\n",
    "        # Compute gradient and update        \n",
    "        loss.backward()         # Compute the gradient\n",
    "        optimizer.step()        # Update the parameters\n",
    "        loss_per_epoch[epoch] = loss.item() / len(X_train) # Store the loss\n",
    "        \n",
    "    return loss_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "29a68ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Cartesian Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, E = get_carbon_cluster_data(n=6) # n is the number of atoms in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "73bb81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can try to take the Cartesian coordinates as descriptors, to do so we will \n",
    "take the $(N, 3)$ matrix of coordinates and flatten it into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6275e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, E_train, X_test, E_test = train_test_split(X, E, test_size=0.5)\n",
    "\n",
    "print('25 examples of (6,3)-matrices', X_train.shape)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "print('25 examples of vectors', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3b465bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "We will use the neural network model defined previously. \n",
    "We can check that this does indeed work as a $\\mathcal{R}^{18} \\rightarrow \\mathcal{R}$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetworkModel(vector_dim=18)\n",
    "E = model(X_train[0:1])\n",
    "print('Output of the model for a single example', E)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "ceb58d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can try training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetworkModel(18)\n",
    "loss_history = training_loop(model, X_train, E_train, epochs=10000, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d897fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.plot(loss_history)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.potentials.plot_parity import plot_parity\n",
    "\n",
    "E_pred_train = model(X_train).detach().numpy()\n",
    "E_pred_test = model(X_test).detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7, 3), layout=\"constrained\")\n",
    "plot_parity(axes[0], E_train, E_pred_train)\n",
    "plot_parity(axes[1], E_test, E_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c3a01c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another metric that is interesting is the loss on the test set as a function of the size of the training set. \n",
    "\n",
    "If the model is capable of learning, the loss on the test set should decrease as more training data is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ef558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [5, 15, 25]\n",
    "losses = []\n",
    "\n",
    "for train_size in sizes:\n",
    "    model = NeuralNetworkModel(18)\n",
    "    loss_history = training_loop(model, X_train[0:train_size], E_train[0:train_size], epochs=10000, lr=0.003)\n",
    "\n",
    "    E_pred_test = model(X_test).detach().numpy()\n",
    "    test_loss = np.sum(E_test.detach().numpy() - E_pred_test)**2\n",
    "    losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c59fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "\n",
    "ax.plot(sizes, losses, 'o-')\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('Test loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "6adcca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Coulomb Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c235a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to build accurate models we need a better descriptor for atomic systems. \n",
    "An early proposal for such a descriptor is the *Coulomb matrix* given by\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C_{ij} =\n",
    "    \\begin{cases}\n",
    "        Z_i^2 & \\text{if } i = j \\\\\n",
    "        \\frac{Z_iZ_j}{r_{ij}^2} & \\text{if } i \\neq j\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $Z_i$ is the atomic number of atom $i$ and $r_{ij}$ is the distance between atoms $i$ and $j$.\n",
    "\n",
    "\n",
    "Consider an atomic configuration with 4 atoms of the same species, then the Coulomb matrix is given by\n",
    "\n",
    "$$\n",
    "C = Z^2 \\begin{bmatrix}\n",
    "1 & r_{12}^{-2} & r_{13}^{-2} & r_{14}^{-2} \\\\\n",
    "r_{21}^{-2} & 1 & r_{23}^{-2} & r_{24}^{-2} \\\\\n",
    "r_{31}^{-2} & r_{32}^{-2} & 1 & r_{34}^{-2} \\\\\n",
    "r_{41}^{-2} & r_{42}^{-2} & r_{43}^{-2} & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want a vector so one possibility is to just flatten the matrix e.g. we make a \n",
    "vector $v_C$\n",
    "\n",
    "$$\n",
    "v_C = [C_{11}, C_{12}, ... , ... , C_{NN}] \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coulomb_matrix(X):\n",
    "    n = X.shape[0]\n",
    "    C = np.zeros((n, n))\n",
    "    Z = 6\n",
    "    # Compute the Coulomb matrix - you can e.g. use a nested loop\n",
    "\n",
    "    return C.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, E = get_carbon_cluster_data(n=8) # n is the number of atoms in the cluster\n",
    "X_train, E_train, X_test, E_test = train_test_split(X, E, test_size=0.5)\n",
    "\n",
    "# Compute the Coulomb matrix for the training and test sets\n",
    "X_train = torch.tensor(np.array([get_coulomb_matrix(x) for x in X_train])).float()\n",
    "X_test = torch.tensor(np.array([get_coulomb_matrix(x) for x in X_test])).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "34d90131",
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to make a model using this descriptor we need to know how many dimensions it has, \n",
    "before our Cartesian descriptor has 18 dimensions we instantiated our model like `NeuralNetworkModel(vector_dim=18)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetworkModel(vector_dim=64)\n",
    "loss_history = training_loop(model, X_train, E_train, epochs=25000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.plot(loss_history)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_pred_train = model(X_train).detach().numpy()\n",
    "E_pred_test = model(X_test).detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7, 3), layout=\"constrained\")\n",
    "plot_parity(axes[0], E_train, E_pred_train)\n",
    "plot_parity(axes[1], E_test, E_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4bb4462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "There is clearly some outliers, but for a lot of the test set the Coulomb matrix description does get it pretty correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "7fb27bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: What is invariance anyway? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.potentials.load_carbon_data import get_invariances_examples\n",
    "from agox.utils.plot import plot_atoms, plot_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "94edbbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "To get a feel for this we will look at two examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_atoms, ring_atoms = get_invariances_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "46160472",
   "metadata": {},
   "outputs": [],
   "source": [
    "We want some way of comparing how similar these two structures are, there are many \n",
    "possibilities, but one fairly intuitive way is\n",
    "\n",
    "$$\n",
    "d(\\vec{v_i}, \\vec{v_j}) = \\frac{\\vec{v_i}\\cdot\\vec{v_j}}{|\\vec{v_i}||\\vec{v_j}|}\n",
    "$$\n",
    "This metric will measure the similarity between two vectors and always return a value \n",
    "between 0 and 1 - with a value of one meaning that the vectors are identical e.g. \n",
    "$d(v_i, v_i) = 1$.\n",
    "\n",
    "Implement a function `dot_product_similarity` to calculate this. \n",
    "\n",
    "Hint: You may find `np.dot` and `np.linalg.norm` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_similarity(v1, v2):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95af7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = chain_atoms.get_positions().flatten() # 18-dimensional vector\n",
    "X2 = ring_atoms.get_positions().flatten() # 18-dimensional vector\n",
    "\n",
    "d11 = dot_product_similarity(X1, X1)\n",
    "d22 = dot_product_similarity(X2, X2)\n",
    "d12 = dot_product_similarity(X1, X2)\n",
    "print(f'Similarity chain vs chain: {d11:.3f}')\n",
    "print(f'Similarity ring vs ring: {d11:.3f}')\n",
    "print(f'Similarity ring vs chain: {d12:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2c724e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "So the vector of Cartesian coordinates are different, as expected, so an algorithm \n",
    "does get a signal that these structures are not the same. \n",
    "\n",
    "However, it seems like a small difference - so lets try to investigate further.\n",
    "To do so we will consider transformations under which the energy of the system is invariant. \n",
    "Specifically\n",
    "\n",
    "- **Translation**: Rigid movement of the entire structure. \n",
    "\n",
    "- **Rotation**: A rotation of the structure.\n",
    "\n",
    "- **Permutation**: Changing the order of the rows of the Cartesian coordinates.\n",
    "\n",
    "The cells below creates a plot of transformed version of the chain and ring structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.potentials.load_carbon_data import transforms\n",
    "\n",
    "atoms_dict = {}\n",
    "for base_atoms, base_label in zip([chain_atoms, ring_atoms], ['chain', 'ring']):\n",
    "    for transform_label, transform in transforms.items():\n",
    "        atoms_dict[f\"{base_label}-{transform_label}\"] = transform(base_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51bf4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(8, 4), layout=\"constrained\")\n",
    "\n",
    "for ax, (label, atoms) in zip(axes.flatten(), atoms_dict.items()):\n",
    "\n",
    "    plot_atoms(ax, atoms)\n",
    "    plot_cell(ax, atoms.cell, collection_kwargs={'alpha': 0.1})\n",
    "\n",
    "    for atom in atoms:\n",
    "        ax.text(atom.position[0], atom.position[1], atom.index, va='center', ha='center')\n",
    "\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "99a0b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "We have 8 configurations here, so what we will do is make a 8x8 matrix that contains \n",
    "the similarity measure between each combination of these eight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3526409",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((len(atoms_dict), len(atoms_dict)))\n",
    "             \n",
    "for i, atoms_1 in enumerate(atoms_dict.values()):\n",
    "    for j, atoms_2 in enumerate(atoms_dict.values()):\n",
    "        v1 = atoms_1.get_positions().flatten()\n",
    "        v2 = atoms_2.get_positions().flatten()\n",
    "        similarity = dot_product_similarity(v1, v2)\n",
    "        D[i, j] = similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d19ad50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can then visualize this matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "cax = ax.matshow(D, vmin=D.min()*0.90, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(len(atoms_dict)))\n",
    "ax.set_yticks(range(len(atoms_dict)))\n",
    "ax.set_xticklabels(atoms_dict.keys(), rotation=90)\n",
    "ax.set_yticklabels(atoms_dict.keys())\n",
    "\n",
    "for i in range(len(atoms_dict)):\n",
    "    for j in range(len(atoms_dict)):\n",
    "        ax.text(j, i, f'{D[i, j]:.2f}', ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "# Red box around upper 4x4\n",
    "rect = plt.Rectangle((-0.5, -0.5), 4, 4, edgecolor='red', facecolor='none', linewidth=2, clip_on=False, zorder=2)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Blue box around lower 4x4\n",
    "rect = plt.Rectangle((3.5, 3.5), 4, 4, edgecolor='blue', facecolor='none', linewidth=2, clip_on=False, zorder=2)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "15be223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "This might be a little complicated to decipher, however here are some key points: \n",
    "\n",
    "- The diagonal is the self-similarity and is 1.0 - as we expect everything is exactly similar to itself.\n",
    "\n",
    "- The (4x4)-matrix on the upper-left, marked with red, is the chain compared to itself with different transforms applied. Ideally we would want all these to be 1.0 such that our machine learning algorithm seems these as identical. \n",
    "\n",
    "- The same applies to the (4x4)-matrix on the lower-right, marked with blue, which is for the ring structure and its transformed copies. \n",
    "\n",
    "- The two off-diagonal (4x4)-matrices are then for the ring vs chain - here we see another manifestation of the problem with using the Cartesian coordinates - the ring and the chain are comparably similar, similarties around 0.95, than a rotated copy of the chain compared to the original chain with a similarity of 0.94."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "ab5569d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Make the similarity matrix but use the Coulomb matrix rather than the Cartesian coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_configurations = len(atoms_dict)\n",
    "D = np.zeros((N_configurations, N_configurations))\n",
    "             \n",
    "for i, atoms_1 in enumerate(atoms_dict.values()):\n",
    "    for j, atoms_2 in enumerate(atoms_dict.values()):\n",
    "        v1 = 1 # Compute the Coulomb matrix for atoms_1\n",
    "        v2 = 1 # Compute the Coulomb matrix for atoms_2\n",
    "        similarity = dot_product_similarity(v1, v2)\n",
    "        D[i, j] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eddb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "cax = ax.matshow(D, vmin=D.min()*0.90, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(len(atoms_dict)))\n",
    "ax.set_yticks(range(len(atoms_dict)))\n",
    "\n",
    "ax.set_xticklabels(atoms_dict.keys(), rotation=90)\n",
    "ax.set_yticklabels(atoms_dict.keys())\n",
    "\n",
    "for i in range(len(atoms_dict)):\n",
    "    for j in range(len(atoms_dict)):\n",
    "        ax.text(j, i, f'{D[i, j]:.2f}', ha='center', va='center', color='black', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4ff4095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Notice that we know have two (3x3)-blocks that are exactly 1.0! \n",
    "\n",
    "This is because the Coulomb matrix is **invariant** to rotation and translation. \n",
    "That is, it doesn't change if the molecule is translated or rotated. \n",
    "\n",
    "However, we can see that it is not invariant to a permutation of the atoms! \n",
    "\n",
    "There are at least two ways of making a permutation invariant version of the Coulomb matrix\n",
    "\n",
    "1. Before flattening to a vector sort the rows and the columns by their norm. E.g. such that the \n",
    "first row of the matrix is the row with the longest norm, the second the second longest and so on - and \n",
    "equivalently for the rows. \n",
    "\n",
    "2. Rather than using the matrix itself, one can use its eigenvalues sorted from smallest to largest (or vice versa). \n",
    "This is, somewhat, analagous to comparing two molecules by their energy levels that appear as the eigenvalues of an \n",
    "Hamiltonian. \n",
    "\n",
    "The function `sort_matrix` performs the sorting required for the first option. \n",
    "Your task is to implement the `get_sorted_coulomb_matrix`-function in the next cell\n",
    "after that and make the plot of the similarity matrix with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6128275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_matrix(matrix):\n",
    "    row_norms = np.linalg.norm(matrix, axis=1)\n",
    "    col_norms = np.linalg.norm(matrix, axis=0)\n",
    "\n",
    "    row_order = np.argsort(row_norms)\n",
    "    col_order = np.argsort(col_norms)\n",
    "\n",
    "    return matrix[row_order][:, col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c36245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_coulomb_matrix(X):    \n",
    "    pass # Implement the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "cax = ax.matshow(D, vmin=D.min()*0.90, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(len(atoms_dict)))\n",
    "ax.set_yticks(range(len(atoms_dict)))\n",
    "\n",
    "ax.set_xticklabels(atoms_dict.keys(), rotation=90)\n",
    "ax.set_yticklabels(atoms_dict.keys())\n",
    "\n",
    "for i in range(len(atoms_dict)):\n",
    "    for j in range(len(atoms_dict)):\n",
    "        ax.text(j, i, f'{D[i, j]:.2f}', ha='center', va='center', color='black', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a808fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "You should now have that this is four blocks: \n",
    "\n",
    "1. Upper-left: All versions of the chain are completely similar. \n",
    "\n",
    "2. Lower-right: All versions of the ring are completely similar. \n",
    "\n",
    "3. Lower-left / Upper-right: All comparisons between versions of rings and chains are equally similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "b32b8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Use the sorted Coulomb matrix descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3d1a372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Not giving you any code this time, you can find anything you need in the previous exercises.\n",
    "\n",
    "Do the following; \n",
    "\n",
    "1. Get the dataset and split into train and test.\n",
    "2. Convert the Cartesian coordinates to sorted Coulomb matrix vectors\n",
    "3. Train the `NeuralNetworkModel`\n",
    "4. Analyze the training and the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "6db4c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Bond distance histogram descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "56c447df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another way of making a useful descriptor is a histogram of bond distances.\n",
    "\n",
    "Lets start by literally making a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def bond_histogram_descriptor(X):\n",
    "    n = X.shape[0]\n",
    "    distances = pdist(X)\n",
    "    bins, edges = np.histogram(distances, bins=20, range=(0, 5))\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "013a8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can plot it to compare the ring and the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc791a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_atoms, ring_atoms = get_invariances_examples()\n",
    "\n",
    "chain_histogram = bond_histogram_descriptor(chain_atoms.get_positions())\n",
    "ring_histogram = bond_histogram_descriptor(ring_atoms.get_positions())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "\n",
    "r = np.linspace(0, 5, chain_histogram.shape[0])\n",
    "\n",
    "ax.bar(r, chain_histogram, alpha=0.5, label='Chain', width=r[1]-r[0])\n",
    "ax.bar(r, ring_histogram, alpha=0.5, label='Ring', width=r[1]-r[0])\n",
    "\n",
    "ax.set_xlabel('Bond length')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_configurations = len(atoms_dict)\n",
    "D = np.zeros((N_configurations, N_configurations))\n",
    "             \n",
    "for i, atoms_1 in enumerate(atoms_dict.values()):\n",
    "    for j, atoms_2 in enumerate(atoms_dict.values()):\n",
    "        v1 = bond_histogram_descriptor(atoms_1.get_positions())\n",
    "        v2 = bond_histogram_descriptor(atoms_2.get_positions())\n",
    "        similarity = dot_product_similarity(v1, v2)\n",
    "        D[i, j] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfa538",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "cax = ax.matshow(D, vmin=D.min()*0.90, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(len(atoms_dict)))\n",
    "ax.set_yticks(range(len(atoms_dict)))\n",
    "\n",
    "ax.set_xticklabels(atoms_dict.keys(), rotation=90)\n",
    "ax.set_yticklabels(atoms_dict.keys())\n",
    "\n",
    "for i in range(len(atoms_dict)):\n",
    "    for j in range(len(atoms_dict)):\n",
    "        ax.text(j, i, f'{D[i, j]:.2f}', ha='center', va='center', color='black', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a8700bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The similarity matrix is good, so we can try using it for training our model. \n",
    "\n",
    "Train a model using it below, as before the required steps are \n",
    "\n",
    "1. Make `X_train` and `X_test` using the `bond_histogram_descriptor`\n",
    "\n",
    "2. Make an instance of the `NeuralNetworkModel` and train it usding the `training_loop`.\n",
    "\n",
    "3. Analyze the results by making parity plots / plotting the loss history."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4e326c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "You might not get really great results. \n",
    "\n",
    "One reason for this is that the histogram is not a smooth function, if a \n",
    "bond distance grows the count in one bin decreases suddenly and the count in \n",
    "another increases suddenly.\n",
    "\n",
    "We can make a better version, to do so we will place a Gaussian in each bin and sum \n",
    "over the contributions from all distances and we will also apply a smooth cutoff function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprint(positions, cutoff=5.0, n=16, sigma=0.5):\n",
    "    R = pdist(positions).reshape(-1, 1)\n",
    "    r0 = np.linspace(0, cutoff, n).reshape(1, n)\n",
    "    z = np.exp(-(R-r0)**2/ sigma**2) * np.cos(np.pi/2 * r0 / cutoff) * (r0 < cutoff)\n",
    "    return z.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199afa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_atoms, ring_atoms = get_invariances_examples()\n",
    "\n",
    "chain_fingerprint = fingerprint(chain_atoms.get_positions(), n=20, sigma=0.5).flatten()\n",
    "ring_fingerprint = fingerprint(ring_atoms.get_positions(), n=20, sigma=0.5).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "\n",
    "r = np.linspace(0, 5, chain_fingerprint.shape[0])\n",
    "\n",
    "ax.bar(r, chain_fingerprint, alpha=0.5, label='Chain', width=r[1]-r[0])\n",
    "ax.bar(r, ring_fingerprint, alpha=0.5, label='Ring',  width=r[1]-r[0])\n",
    "\n",
    "ax.set_xlabel('Bond length')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "acce1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the fingerprint descriptor to train a model and make parity plots for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, E = get_carbon_cluster_data(n=8) # n is the number of atoms in the cluster\n",
    "X_train, E_train, X_test, E_test = train_test_split(X, E, test_size=0.5)\n",
    "\n",
    "# Compute the Coulomb matrix for the training and test sets\n",
    "X_train = torch.tensor(np.array([fingerprint(x, n=16, sigma=0.5) for x in X_train])).float()\n",
    "X_test = torch.tensor(np.array([fingerprint(x, n=16, sigma=0.5) for x in X_test])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481afb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and train model using the fingerprint data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "ccad6784",
   "metadata": {},
   "outputs": [],
   "source": [
    "With this you should be able to get fairly decent results on the test set.\n",
    "\n",
    "Controlling overfitting is somewhat difficult though! But you can investigate \n",
    "\n",
    "- How the number of bins influences the fit\n",
    "\n",
    "- How the `sigma` parameter of the fingerprint influences the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3012d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bonus Exercise: Using a different model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "479cad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "If you have gotten this far, you can try implementing the same type of model \n",
    "that the Coulomb matrix was originally used for. \n",
    "\n",
    "You can find the details in the paper: [Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning](https://arxiv.org/abs/1109.2618).\n",
    "\n",
    "Specifically equations (2) and (3). "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
