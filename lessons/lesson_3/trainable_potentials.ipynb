{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ccd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install\n",
    "try:\n",
    "  import imlms\n",
    "  print('Already installed')\n",
    "except:\n",
    "  %pip install git+https://github.com/Mads-PeterVC/imlms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(install.stdout.splitlines()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eecea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "66e0f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainable Interatomic Potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "482d9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Primer: A few more torch tips & tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4fb2a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "So far when when trained models using gradient descent we have done it in the most \n",
    "explicit way - but not the most convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c091ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, a, b):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor(a))\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
    "    def forward(self, x):\n",
    "        return self.a * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.torch import get_example_data\n",
    "\n",
    "# Same data as in the previous example from lesson 2\n",
    "X, y = get_example_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "007cf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Again we want to train the model, but this time we use\n",
    "\n",
    "- An `optimizer`: This class handles everything about updating the model parameters. \n",
    "- A `loss`-function, because we are use the very common mean-squared-error we can take that directly from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3201c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "model = LinearModel(a=0.5, b=0.5)\n",
    "y_pred_initial = model(X) # Predictions with the initial model\n",
    "\n",
    "# Create an optimizer, the optimizer is told the parameters to optimize e.g. all the parameters of the model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# If we want to optimize only a subset of the parameters we can do so by passing \n",
    "# a list of parameters to the optimizer. \n",
    "# For example we can optimize only the parameter a of the model, controlling the slope.\n",
    "# optimizer = torch.optim.SGD([model.a], lr=0.01) # <--- Try this too!!\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Run the optimization loop\n",
    "for i in range(300):\n",
    "\n",
    "    # Zero the gradients - this is needed because PyTorch otherwise accumulates the gradients.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: Make predictions and calculate the loss\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    # Backward pass: Calculate the gradients - this is instead of the \n",
    "    # torch.autograd.grad() function we used in the previous example\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters by taking a step with the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss: {l.item()}')\n",
    "\n",
    "y_pred_final = model(X)\n",
    "\n",
    "print(f'Final model: a={model.a.item():0.2f}, b={model.b.item():0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax.plot(X.detach().numpy(), y_pred_initial.detach().numpy(), label=\"Initial\")\n",
    "    ax.plot(X.detach().numpy(), y_pred_final.detach().numpy(), label=\"Final\")\n",
    "    ax.scatter(X.detach().numpy(), y.detach().numpy(), label=\"Data\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "b2f09377",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Trainable Lennard Jones Potential"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9c7df50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "What we want to do now is to make the parameters `sigma` and `epsilon` learnable from \n",
    "data. This will enable us to fit the Lennard Jones potential as best as possible to \n",
    "a given dataset. \n",
    "\n",
    "To do this we will change a few things from our `LennardJonesTorch` class, \n",
    "we will make another called `LennardJonesModule` with these changes.\n",
    "\n",
    "1. `LennardJonesModule` should *inherit* from `torch.nn.Module`. \n",
    "2. The parameters should be set as `torch.nn.Parameter` instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LennardJonesModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, sigma=1.0, epsilon=1.0):\n",
    "        super().__init__()\n",
    "        self.sigma = torch.nn.Parameter(torch.tensor(sigma))\n",
    "        self.epsilon = torch.nn.Parameter(torch.tensor(epsilon))\n",
    "\n",
    "    def get_lj_energy(self, r):\n",
    "        # Calculate the Lennard-Jones energy using the class attributes \n",
    "        # self.sigma and self.epsilon.\n",
    "        return 4 * self.epsilon * ((self.sigma/r)**12 - (self.sigma/r)**6)\n",
    "\n",
    "    def forward(self, distances):\n",
    "        \"\"\"\n",
    "        The 'distances'-tensor has shape (N, B) where N is the number of example\n",
    "        configurations and B is the number of bonds in each configuration. \n",
    "        \"\"\"\n",
    "        # Sum the Lennard-Jones energy over all bonds in each configuration to get the total energy.\n",
    "        E = torch.sum(self.get_lj_energy(distances), axis=1)\n",
    "        return E"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c99b6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "I have created a function that produces a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.potentials import get_simple_lj_dataset\n",
    "\n",
    "# # Generate a dataset\n",
    "R, y = get_simple_lj_dataset(n_samples=25)\n",
    "\n",
    "print(f\"{R.shape = }\")\n",
    "print(f\"{y.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "aacdbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The dataset contains 25 examples of dimers where\n",
    "\n",
    "- `R`: The bond distances of the example configuration\n",
    "- `y`: The total energy of the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74736b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_init = np.random.uniform(1.0, 2.0)\n",
    "epsilon_init = np.random.uniform(1.0, 2.0)\n",
    "\n",
    "model = LennardJonesModule(sigma=sigma_init, epsilon=epsilon_init)\n",
    "\n",
    "# Define the optimizer: Different optimizer than in the previous example, but the same principle\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    loss = 0\n",
    "    y_pred = model(R)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "16f170e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "If the loss doesn't really get very low try running the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "\n",
    "with torch.no_grad():\n",
    "    R_cont = torch.linspace(R.min(), R.max(), 100).reshape(-1, 1)\n",
    "    y_pred = model(R_cont)\n",
    "    ax.plot(R_cont, y_pred, label='Predicted')\n",
    "\n",
    "# Plot the true vs predicted energies\n",
    "r_flat = R.flatten()\n",
    "y_flat = y.flatten()\n",
    "ax.plot(r_flat, y_flat, 'o')\n",
    "\n",
    "ax.set_xlabel('r [Å]')\n",
    "ax.set_ylabel('E [eV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "121bcf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Learning from clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cca073f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the previous example the training data we used consisted of dimers and our model is a two-body model, \n",
    "so we could easily fit the data. \n",
    "\n",
    "Now things will be a little more difficult, our new dataset with consist of clusters with 12 atoms, \n",
    "but we still want to fit it with the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.potentials.load_cluster_data import get_cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, R, E = get_cluster_data(train=True)\n",
    "\n",
    "print(f\"{X.shape = }\")\n",
    "print(f\"{R.shape = }\")\n",
    "print(f\"{E.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_init = np.random.uniform(1.0, 2.0)\n",
    "epsilon_init = np.random.uniform(1.0, 2.0)\n",
    "\n",
    "model = LennardJonesModule(sigma=sigma_init, epsilon=epsilon_init)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = None # <--- Change this\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Write loop body here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, R_test, E_test = get_cluster_data(train=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    E_train_pred = model(R)\n",
    "    E_test_pred = model(R_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d540ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to visualize the learning process we need to do something new this time, \n",
    "as the dataset isn't a nice 1D function we can just plot.\n",
    "\n",
    "Instead we will create a *parity plot* where each for each datapoint we plot \n",
    "the true value vs the predicted value. If the model is perfect all the points \n",
    "should fall on a straight line. \n",
    "\n",
    "Another thing we are doing now is that we will make plots for the dataset we \n",
    "trained on and a dataset that we kept hidden from the model. This will help us \n",
    "detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), layout='constrained')\n",
    "\n",
    "labels = ['Train', 'Test']\n",
    "for ax, E_true, E_pred, label in zip(axes, [E, E_test], [E_train_pred, E_test_pred], labels):\n",
    "\n",
    "    ax.plot(E_true, E_pred, 'o')\n",
    "\n",
    "    ax.set_xlabel('True energy [eV]')\n",
    "    ax.set_ylabel('Predicted energy [eV]')\n",
    "\n",
    "    E_min = min(E.min(), E_pred.min()) - 1\n",
    "    E_max = max(E.max(), E_pred.max()) + 1    \n",
    "    ax.plot([E_min, E_max], [E_min, E_max], 'k--')\n",
    "    ax.set_xlim(E_min, E_max)\n",
    "    ax.set_ylim(E_min, E_max)\n",
    "\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fde972b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can generally not get a perfect fit, the cloud of points certainly follows the straight line \n",
    "but there are lots of places where the predictions are very wrong! "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "08ac3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: More expressive potential"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0a92653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Our Lennard Jones potential from above is very limited in its form, so we can try making a \n",
    "model that has a little more freedom. \n",
    "\n",
    "In this case I will choose that we will add a number of Gaussians with some learnable parameters \n",
    "to the Lennard Jones predictions. To do so we will implemenet a `GaussianModule`-class. \n",
    "\n",
    "Recall that a Gaussian function is defined by \n",
    "\n",
    "$$\n",
    "f(x) = A \\exp{\\left(-\\frac{(r-r_0)^2}{2\\sigma^2}\\right)}\n",
    "$$\n",
    "Where the parameters are\n",
    "\n",
    "- $A$: Controlling the amplitude\n",
    "- $r_0$: Controlling the location\n",
    "- $\\sigma$: Controlling the width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, r0, A, sigma):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.tensor(A))\n",
    "        self.sigma = torch.nn.Parameter(torch.tensor(sigma))\n",
    "\n",
    "        # I decided to make r0 a constant parameter instead of a learnable parameter\n",
    "        # but you can play around with this if you want to - just remove the requires_grad=False.\n",
    "        self.r0 = torch.nn.Parameter(torch.tensor(r0), requires_grad=False) \n",
    "\n",
    "    def forward(self, r):\n",
    "        # Implement the forward pass here\n",
    "        f = 0 # <--- Change this\n",
    "        return f.sum(axis=1) # This sums the energies over all the bonds in each configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d299f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next we can create a new model that uses both our `LennardJonesModule` and our `GaussianModule`. \n",
    "\n",
    "Torch makes this simple, we can just set the modules as properties of our new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, N_basis, r_max, sigma_lj, epsilon_lj):\n",
    "        super().__init__()\n",
    "        # Create Gaussians with centers from 0 to r_max\n",
    "        self.rbf = torch.nn.ModuleList([GaussianModule(r0.item(), 0.1, 0.1) for r0 in torch.linspace(0, r_max, N_basis)])\n",
    "\n",
    "        # Create the Lennard-Jones module\n",
    "        self.lj = LennardJonesModule(sigma=sigma_lj, epsilon=epsilon_lj)\n",
    "\n",
    "    def forward(self, R):\n",
    "        # Calculate the Lennard-Jones energy\n",
    "        E = self.lj(R)\n",
    "\n",
    "        # Calculate the contribution from each Gaussian\n",
    "        for rbf in self.rbf:\n",
    "            E += rbf(R)\n",
    "        return E"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "391437fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "The training loop is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, R, E = get_cluster_data(train=True)\n",
    "\n",
    "sigma_init = 1.0\n",
    "epsilon_init = 1.0\n",
    "\n",
    "model = Model(N_basis=10, r_max=3.0, sigma_lj=sigma_init, epsilon_lj=epsilon_init)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 5000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    loss = 0\n",
    "    y_pred = model(R)\n",
    "    loss = loss_fn(y_pred, E)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, R_test, E_test = get_cluster_data(train=False)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    E_train_pred = model(R)\n",
    "    E_test_pred = model(R_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "26d7f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "We again make parity-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a22c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), layout='constrained')\n",
    "\n",
    "for ax, E_true, E_pred in zip(axes, [E, E_test], [E_train_pred, E_test_pred]):\n",
    "\n",
    "    ax.plot(E_true, E_pred, 'o')\n",
    "\n",
    "    ax.set_xlabel('True energy [eV]')\n",
    "    ax.set_ylabel('Predicted energy [eV]')\n",
    "\n",
    "    E_min = min(E.min(), E_pred.min()) - 1\n",
    "    E_max = max(E.max(), E_pred.max()) + 1\n",
    "\n",
    "    ax.plot([E_min, E_max], [E_min, E_max], 'k--')\n",
    "    ax.set_xlim(E_min, E_max)\n",
    "    ax.set_ylim(E_min, E_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "aa143edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "You should be able to get parity plots that are nearly perfect, with all points being very close to the straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3894b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can inspect the form of the potential that our model has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "\n",
    "R = torch.linspace(0.8, 6.0, 100)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    E = model(R.reshape(-1, 1))\n",
    "    ax.plot(R, E)\n",
    "\n",
    "    ax.set_xlabel('r [Å]')\n",
    "    ax.set_ylabel('E [eV]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d5f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
