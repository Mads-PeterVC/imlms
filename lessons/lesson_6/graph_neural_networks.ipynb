{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install\n",
    "try:\n",
    "  import imlms\n",
    "  print('Already installed')\n",
    "except:\n",
    "  %pip install git+https://github.com/Mads-PeterVC/imlms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(install.stdout.splitlines()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "04a4bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pytorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "\n",
    "from matscipy.neighbours import neighbour_list\n",
    "from ase.data.colors import jmol_colors\n",
    "\n",
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "80b9ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Warning: This tutorial is probably a less pedagogical and more difficult than the other ones. But there is not really any code you need to write, so you can just run everything and explore at your own pace."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "93937b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graphs in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "75cc9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that we have gotten a feeling for the basic operations of a `torch` we are \n",
    "ready to take a look at more advanced network types. \n",
    "\n",
    "The state of the art neural networks for fitting potentials (and other tasks in material science) are graph neural networks.\n",
    "\n",
    "So it is of interest to understand how to work with graph neural networks. \n",
    "\n",
    "Remember that a graph consists of\n",
    "* $V$: Vertices (or nodes).\n",
    "* $E$: Edges. \n",
    "\n",
    "And optionally\n",
    "* Vertex features: Such as the atomic number or an embedding thereof. \n",
    "* Edge features: Such as the distance between the two connected vertices. \n",
    "\n",
    "It can be quite helpful to store these properties in an appropriate data structure, such as the graph structure provided by `torch_geometric`. \n",
    "\n",
    "Lets make a simple graph using `torch_geometric.data.Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08678b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long) # Edges \n",
    "\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float) # Vertex features.\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a4463fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "This graph looks like this\n",
    "\n",
    "![graph_example](https://pytorch-geometric.readthedocs.io/en/latest/_images/graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f584d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "The cell below defines two functions, one that creates a random graph and one that plots the graph. \n",
    "\n",
    "The random graph created has one attribute, namely an integer number between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.get_cmap('coolwarm')\n",
    "norm = plt.Normalize(0, 9)\n",
    "\n",
    "# Create a random graph: \n",
    "def random_graph(num_nodes, cutoff=2.5, box_size=10):\n",
    "\n",
    "    positions = []\n",
    "    for i in range(num_nodes):\n",
    "\n",
    "        new_position = torch.rand(1, 2) * box_size\n",
    "\n",
    "        if len(positions) > 0:\n",
    "            all_positions = torch.vstack(positions)\n",
    "            while torch.any(torch.cdist(all_positions, new_position) < 0.75):\n",
    "                new_position = torch.rand(1, 2) * box_size\n",
    "        \n",
    "        positions.append(new_position)\n",
    "\n",
    "    positions = torch.vstack(positions)\n",
    "    D = torch.cdist(positions, positions)\n",
    "\n",
    "    edge_index = []\n",
    "    for i in range(D.shape[0]):\n",
    "        for j in range(D.shape[1]):\n",
    "            if D[i, j] < cutoff:\n",
    "                edge_index.append([i, j])\n",
    "\n",
    "    edge_index = torch.tensor(np.array(edge_index).T, dtype=torch.int64).reshape(2, -1)\n",
    "\n",
    "    x = torch.randint(0, 10, (num_nodes, 1))\n",
    "\n",
    "    return Data(edge_index=edge_index, pos=positions, x=x)\n",
    "\n",
    "def plot_graph(ax, graph):\n",
    "    positions = graph.pos.detach().numpy()\n",
    "    numbers = graph.x.detach().numpy()\n",
    "\n",
    "    # Plot the nodes:\n",
    "    r = 0.35\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "\n",
    "    # Plot the edges:\n",
    "    for idx, edge in enumerate(graph.edge_index.T):\n",
    "        source = positions[edge[0]]\n",
    "        target = positions[edge[1]]\n",
    "\n",
    "        # Plot the target node:\n",
    "        ax.plot(target[0]+x, target[1]+y, c='black')\n",
    "        ax.fill_between(target[0]+x, target[1]+y, target[1], color=colors(norm(numbers[edge[1]])))\n",
    "\n",
    "        ax.plot([source[0], target[0]], [source[1], target[1]], c='black', zorder=0)\n",
    "\n",
    "    ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4360fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Message Passing\n",
    "\n",
    "Now we will build a simple message-passing layer to illustrate how it works. In general \n",
    "a message-passing layer can be defined by\n",
    "\n",
    "$$\n",
    "x_i^{k+1} = \\phi \\left( x^k_i, \\bigoplus_{j \\in N_i} \\psi(x^k_i, x^k_j, e_{ji}) \\right)\n",
    "$$\n",
    "\n",
    "This is an intimidating equation at first encounter, so lets break it down;\n",
    "\n",
    "- $x_i^k$ : The initial features of node $i$.\n",
    "- $x_i^k$ : Features of node $i$ after the update.\n",
    "- $\\phi$, $\\psi$ : Differentiable functions, typically represented by neural networks.\n",
    "- $\\bigoplus$ : A *permutation invariant* aggregation operation. Aggregation means an operation such \n",
    "as a sum, an average or a maximum and permutation invariant means that the order in which it is applied to the elements is not influential. \n",
    "- $N_i$ : The set of neighbours of node $i$.\n",
    "- $e_{ji}$ : The edge features of the directed node from $j$ to $i$.\n",
    "\n",
    "As this is our first application of message-passing we will simplify a bit, our first \n",
    "simplification will be to choose that \n",
    "\n",
    "$$\\phi(q, p) = p$$\n",
    "Which means that the outer function $\\phi$ just picks the second input.\n",
    "Additionally we will choose\n",
    "$$\n",
    "\\psi(q, p, z) = \\psi(p) = p\n",
    "$$,\n",
    "that is $psi$ only just ignores $q$ and $e$ and returns $p$. With these choices \n",
    "we can restate the equation from above\n",
    "\n",
    "$$\n",
    "x_i^{k+1} = \\bigoplus_{j \\in N_i} x^k_j\n",
    "$$\n",
    "Now will choose a $\\mathrm{max}$ has our aggregation operation $\\bigoplus$\n",
    "\n",
    "$$\n",
    "x_i^{k+1} = \\max_{j \\in N_i} \\ x^k_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "960a599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we define a message passing layer, using `MessagePassing` from `torch_geometric`. \n",
    "This layer updates the node attribute to be the max of its neighbours attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ff03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxMessage(MessagePassing):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MaxMessage, self).__init__(aggr='max')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "message_passing = MaxMessage()\n",
    "\n",
    "sz = 4\n",
    "graph = random_graph(100, cutoff=1.5, box_size=15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(4*sz, 1*sz))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "\n",
    "    if i != 0:\n",
    "        new_x = message_passing(graph.x, graph.edge_index)\n",
    "        graph.x = new_x\n",
    "\n",
    "    plot_graph(ax, graph)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax.set_title(f'{i} Message Passing Steps')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "858ed110",
   "metadata": {},
   "outputs": [],
   "source": [
    "In these plots a dark red color corresponds to a high node feature value and a dark blue color corresponds to a low node feature value.\n",
    "As more and more propagation steps are taken the graphs become as red as the most red member of the subgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "73145b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Exercises:\n",
    "\n",
    "1. Change the message or the aggregration method (`aggr` in the `__init__` call) in some way. E.g. try with minimum or mean aggregation.\n",
    "\n",
    "2. The constructed graph has self-interactions, e.g. each node sees and sends messages to it self - try turning that off by changing how the `edge_index`-tensor is constructed. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "096f1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Atomic Graphs\n",
    "\n",
    "We would like to work with atomic structures in this format, so we need define which atoms are connected with an edge and decide on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomsGraph(Data):\n",
    "\n",
    "    @classmethod\n",
    "    def from_atoms(cls, atoms, cutoff=5.0, dtype=torch.float, energy=None, forces=None):\n",
    "\n",
    "        # Build the neighbour list:\n",
    "        i, j, S = neighbour_list('ijS', atoms=atoms, cutoff=cutoff)\n",
    "\n",
    "        # Edges: Defines which atoms are connected, first row is the sender, \n",
    "        # second row is the reciever (as the graph is directed).\n",
    "        ij = np.array([i, j])\n",
    "        edge_index = torch.tensor(ij, dtype=torch.long)\n",
    "\n",
    "        # Nodes: The initial node features are just the atomic numbers.\n",
    "        node_feat = torch.tensor(atoms.get_atomic_numbers(), dtype=torch.long).reshape(-1)\n",
    "\n",
    "        # Shift vectors:\n",
    "        shifts = torch.tensor(S, dtype=dtype) # These are integer shifts in the unit cell.\n",
    "        cell = torch.tensor(np.array(atoms.get_cell()), dtype=dtype)\n",
    "        shift_vectors = torch.einsum('ij,jk->ik', shifts, cell) # Convert the shifts to vectors in Ã….\n",
    "\n",
    "        # Positios: Have requires grad because we will want to compute forces.\n",
    "        positions = torch.tensor(atoms.get_positions(), dtype=dtype, requires_grad=True)\n",
    "\n",
    "        # Target properties: Might be neater to keep this on the data loader, but I haven't \n",
    "        # gotten that to work well yet.\n",
    "        if energy is not None:\n",
    "            energy = torch.tensor(energy, dtype=dtype).reshape(1, 1)\n",
    "        if forces is not None:\n",
    "            forces = torch.tensor(forces, dtype=dtype)\n",
    "\n",
    "        return cls(pos=positions,\n",
    "                   edge_index=edge_index, \n",
    "                   x=node_feat,\n",
    "                   shift_vectors=shift_vectors, \n",
    "                   cell=cell, \n",
    "                   energy=energy,\n",
    "                   forces=forces)\n",
    "\n",
    "    def plot(self, ax, node_radius=0.4, plot_cell=True):\n",
    "\n",
    "        positions = self.pos.detach().numpy()\n",
    "        numbers = self.x.detach().numpy()\n",
    "\n",
    "        # Plot the nodes:\n",
    "        r = node_radius\n",
    "        theta = np.linspace(0, 2*np.pi, 100)\n",
    "        x = r * np.cos(theta)\n",
    "        y = r * np.sin(theta)\n",
    "\n",
    "        # Plot the edges:\n",
    "        for idx, edge in enumerate(self.edge_index.T):\n",
    "\n",
    "            shift = self.shift_vectors[idx].detach().numpy()\n",
    "            if shift.any():\n",
    "                linestyle = '--'\n",
    "                marker = 'x'\n",
    "            else:\n",
    "                linestyle = '-'\n",
    "                marker=None\n",
    "\n",
    "\n",
    "            source = positions[edge[0]]\n",
    "            target = positions[edge[1]] + shift\n",
    "\n",
    "            # Plot the target node:\n",
    "            ax.plot(target[0]+x, target[1]+y, c='black')\n",
    "            ax.plot(target[0], target[1], marker=marker, c='red', zorder=2)\n",
    "            ax.fill_between(target[0]+x, target[1]+y, target[1], color=jmol_colors[numbers[edge[1]]])\n",
    "\n",
    "            ax.plot([source[0], target[0]], [source[1], target[1]], c='black', linestyle=linestyle, zorder=0)\n",
    "\n",
    "        if plot_cell:\n",
    "            cell = self.cell.detach().numpy()\n",
    "            ax.plot([0, cell[0, 0]], [0, cell[0, 1]], c='black')\n",
    "            ax.plot([cell[0, 0], cell[0, 0]+cell[1, 0]], [cell[0, 1], cell[0, 1]+cell[1, 1]], c='black')\n",
    "            ax.plot([0, cell[1, 0]], [0, cell[1, 1]], c='black')\n",
    "            ax.plot([cell[1, 0], cell[0, 0]+cell[1, 0]], [cell[1, 1], cell[0, 1]+cell[1, 1]], c='black')\n",
    "\n",
    "        ax.axis('equal')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.build import molecule\n",
    "from ase.build import mx2\n",
    "\n",
    "atoms = molecule('C6H6')\n",
    "atoms.set_cell([[10, 0, 0], [0, 10, 0], [0, 0, 10]])\n",
    "atoms.center()\n",
    "molecule_graph = AtomsGraph.from_atoms(atoms, cutoff=2.0) # You can play with the cutoff to get cool graphs.\n",
    "\n",
    "mos2 = mx2(formula='MoS2', kind='2H', a=3.18, thickness=3.19, size=(1, 1, 1), vacuum=10.0)\n",
    "mos2 = mos2.repeat([3, 3, 1])\n",
    "mos2_graph = AtomsGraph.from_atoms(mos2, cutoff=5.0)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "molecule_graph.plot(axes[0])\n",
    "mos2_graph.plot(axes[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d498b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here the nodes marked with a $\\times$ are periodic images.\n",
    "\n",
    "Note, we are only keeping track of node features in the cell, but they can recieve messages \n",
    "from periodic images. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e3ec6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SchNet in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8e9147ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "With the `AtomsGraph`-class defined we can move on to building a message-passing neural \n",
    "network. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d58e441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SchNet"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "88e9eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we're ready to implement SchNet, so lets recap what happens in SchNet:\n",
    "\n",
    "![SchNet](https://www.researchgate.net/profile/Kristof-Schuett/publication/317954658/figure/fig4/AS:530501098524672@1503492726745/Illustration-of-SchNet-with-an-architectural-overview-left-the-interaction-block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "ac9fb938",
   "metadata": {},
   "outputs": [],
   "source": [
    "The schnet architecture is shown in the first colun of the figure. \n",
    "A few things to note: \n",
    "* The first layer is an `embedding` layer, that takes an atomic number and represents it as a vector with 64 elements. \n",
    "* The embedding is followed by the interaction (message-passing) layers.\n",
    "* `Atomwise` just means fully-connected\n",
    "* Finally the local energy is calculated and summed to yield the total energy.\n",
    "\n",
    "The interaction block consists of two branches (middle column), the left branch is just a skip connection and \n",
    "all the interesting things happen in the right-hand branch.\n",
    "1. The node features are updated with a fully connected layer. \n",
    "2. Messages from other nodes are computed using the `cfconv` layer. \n",
    "3. This is followed by more linear layers and a `softplus` activation function.\n",
    "\n",
    "The `cfconv` layer (right most column) is where message passing is performed. \n",
    "1. The distance between nodes `i` and `j` is expanded in a basis of Gaussians in the `rbf` layer. \n",
    "2. A filter is formed by passing this vector through a series of linear layers\n",
    "3. The filter is multplied with the node features elementwise to produce the message."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e74f0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "We will start by defining the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, max_z=100, hidden_dim=32):\n",
    "        super(NodeEmbedding, self).__init__()\n",
    "        self.n_atom_types = max_z\n",
    "        self.linear = torch.nn.Linear(self.n_atom_types, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, numbers):\n",
    "        x = torch.nn.functional.one_hot(numbers-1, num_classes=self.n_atom_types).float()\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a88c4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "This could be simplified by using `torch.nn.Embedding`, but that is less clear about what is going on. \n",
    "The embedding first transforms the atomic number to a onehot-vector which when passed through the linear layer picks out the \n",
    "specific vector that the element is assigned to. \n",
    "\n",
    "Because of the linear layer this becomes a learnable embedding. \n",
    "\n",
    "Next lets look at the distance expansion part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac26212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, distance_dim=32, cutoff=5.0, gamma=10):\n",
    "        super(DistanceEmbedding, self).__init__()\n",
    "        self.r0 = torch.nn.Parameter(torch.linspace(0, 1.5*cutoff, distance_dim), requires_grad=False)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, distances):\n",
    "        x = torch.exp(-self.gamma*(distances - self.r0)**2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "db14ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "This is relatively simple, the distances are expanded in a basis \n",
    "\n",
    "$e_k(d_{ij}) = \\exp (-\\gamma (d_ij - \\mu_k)^2)$\n",
    "\n",
    "Where we have chosen $\\mu_k$ evenly spaced between 0 and 1.5 the cutoff distance.\n",
    "\n",
    "Torch doesn't have the activation function we want by default, so we can implement that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedSoftPlus(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ShiftedSoftPlus, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(0.5 * torch.exp(x) + 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f53e235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "And now we're ready to define the SchNet message passing layer.\n",
    "\n",
    "Here we use the `MessagePassing`-class from `torch_geometric`, as it is easier to \n",
    "efficiently implement this operation in this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchNetMessage(MessagePassing):\n",
    "\n",
    "    def __init__(self, embedding_dim=32, distance_dim=32, activation_fn=ShiftedSoftPlus):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "        # This is the `CFConv` filter-producing function (CFconv block).\n",
    "        self.filter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(distance_dim, embedding_dim),\n",
    "            activation_fn(),\n",
    "            torch.nn.Linear(embedding_dim, embedding_dim),\n",
    "            activation_fn()\n",
    "        )\n",
    "\n",
    "        # First atom-wise transformation (Interaction block)\n",
    "        self.atom_wise_0 = torch.nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # This is the last atom-wise transformation (Interaction block)\n",
    "        self.atom_wise_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, embedding_dim),\n",
    "            activation_fn(),\n",
    "            torch.nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_cutoff(d, cutoff=5.0):\n",
    "        return 0.5 * (torch.cos(np.pi * d / cutoff) + 1.0)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, distances):\n",
    "\n",
    "        # Calculate the filter: We take care of the cutoff here - ensuring that the filter is zero beyond the cutoff.\n",
    "        cont_filter = self.filter(edge_attr) * self.cosine_cutoff(distances)\n",
    "\n",
    "        # Pass node features through an mlp.\n",
    "        x_out = self.atom_wise_0(x)\n",
    "\n",
    "        # Propagate messages: 'x_out' now contains the new node features.\n",
    "        x_out = self.propagate(edge_index, x=x_out, cont_filter=cont_filter)\n",
    "\n",
    "        # Skip connection & final mlp.\n",
    "        x_out = x + self.atom_wise_1(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def message(self, x_j, cont_filter):\n",
    "        # Message function: This is the `CFConv` operation. \n",
    "        # torch_geometric does fancy stuff behind the scenes to make this efficient, \n",
    "        # and we don't need to worry about indexing.\n",
    "        return x_j * cont_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2cad488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we have all the ingredients we need to create the model, so all thats left to do is to \n",
    "combined them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.nn import SumAggregation\n",
    "\n",
    "class SchNetModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=32, distance_dim=300, n_blocks=1, cutoff=5.0,\n",
    "                 activation_fn=ShiftedSoftPlus):\n",
    "        super(SchNetModel, self).__init__()\n",
    "        self.node_embedding = NodeEmbedding(hidden_dim=embedding_dim)\n",
    "        self.distance_embedding = DistanceEmbedding(distance_dim=distance_dim, cutoff=cutoff)\n",
    "\n",
    "        self.sch_blocks = torch.nn.ModuleList([\n",
    "            SchNetMessage(embedding_dim=embedding_dim, \n",
    "                          distance_dim=distance_dim, \n",
    "                          activation_fn=activation_fn)\n",
    "                          for _ in range(n_blocks)\n",
    "            ])\n",
    "        \n",
    "        self.energy_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, embedding_dim),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.Linear(embedding_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.aggr = SumAggregation()\n",
    "\n",
    "    def forward(self, data, compute_forces=False):\n",
    "        x = self.node_embedding(data.x) # Embed the atomic numbers.\n",
    "\n",
    "        # Edge vectors: The vectors between the atoms.\n",
    "        distance_vectors = data.pos[data.edge_index[1]] - data.pos[data.edge_index[0]] + data.shift_vectors\n",
    "\n",
    "        # We only use the radial information, so we compute the distances between the atoms.\n",
    "        distances = torch.norm(distance_vectors, dim=1).reshape(-1, 1)\n",
    "\n",
    "        # Expand the distances in a basis. \n",
    "        edge_attr = self.distance_embedding(distances) \n",
    "\n",
    "        for block in self.sch_blocks: # Apply the SchNet blocks.\n",
    "            x = block(x, data.edge_index, edge_attr, distances) # Iteratively update the node embeddings.\n",
    "\n",
    "        E_atomic = self.energy_head(x) # Compute the atomic energies.\n",
    "        E_total = self.aggr(E_atomic, data.batch)\n",
    "\n",
    "        if compute_forces:\n",
    "            # We sum the energy of all structures and let torch autograd compute the gradients, which takes into \n",
    "            # account which atoms contributed to which energy and thus the forces.\n",
    "            forces = -torch.autograd.grad(E_total.sum(), data.pos, create_graph=True, retain_graph=True)[0] # Compute the forces.\n",
    "            return E_total, forces\n",
    "\n",
    "        return E_total    "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "05f72216",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Morse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.calculators.morse import MorsePotential\n",
    "from ase import Atoms\n",
    "\n",
    "def get_atoms(r):\n",
    "    atoms = Atoms('H2', positions=[[0, 0, 0], [0, 0, r]])\n",
    "    atoms.set_cell(np.eye(3) * 10)\n",
    "    atoms.center()\n",
    "    atoms.calc = MorsePotential()\n",
    "    F = atoms.get_forces()\n",
    "    E = atoms.get_potential_energy()\n",
    "    return atoms, E, F\n",
    "\n",
    "def get_morse_dataset(n_data, r_max=2.0, r_min=0.85):\n",
    "\n",
    "    r_values = np.linspace(r_max, r_min, n_data)\n",
    "    atoms = [None for _ in range(n_data)]\n",
    "    E = np.zeros(n_data)\n",
    "    F = np.zeros((n_data, 2, 3))\n",
    "\n",
    "    for i, r in enumerate(r_values):\n",
    "        atoms[i], E[i], F[i] = get_atoms(r)\n",
    "\n",
    "    return atoms, E, F, r_values\n",
    "\n",
    "atoms, E, F, r_values = get_morse_dataset(100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(r_values, E, '-o', label='Energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cd8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "\n",
    "# Define the dataset with a loader:\n",
    "atoms, E, F, r_values = get_morse_dataset(32)\n",
    "graphs = [AtomsGraph.from_atoms(atoms, cutoff=5.0, energy=e, forces=f) for atoms, e, f in zip(atoms, E, F)]\n",
    "loader = DataLoader(graphs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate a model:\n",
    "model = SchNetModel(n_blocks=1, embedding_dim=32, distance_dim=256)\n",
    "\n",
    "# Parameters:\n",
    "print('Parameters')\n",
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'\\t{name}: {param.shape}, {param.numel()}')\n",
    "    total += param.numel()\n",
    "print(f'Total parameters: {total}')\n",
    "\n",
    "# Make an optimizer: \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss function:\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    for graph_batch in loader:\n",
    "\n",
    "        energy_batch = graph_batch.energy\n",
    "        force_batch = graph_batch.forces\n",
    "\n",
    "        E_pred, F_pred = model(graph_batch, compute_forces=True)\n",
    "        E_loss = loss(E_pred, energy_batch)\n",
    "        F_loss = loss(F_pred, force_batch)\n",
    "\n",
    "        total_loss = E_loss + F_loss\n",
    "        model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_E, test_F, test_r = get_morse_dataset(100)\n",
    "test_graphs = [AtomsGraph.from_atoms(atoms, cutoff=5.0, energy=e, forces=f) for atoms, e, f in zip(test_data, test_E, test_F)]\n",
    "\n",
    "test_loader = DataLoader(test_graphs, batch_size=1, shuffle=False)\n",
    "\n",
    "test_pred = []\n",
    "for graph_batch in test_loader:\n",
    "    test_pred.append(model(graph_batch).item())\n",
    "\n",
    "test_pred = np.array(test_pred).flatten()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(r_values, E, 'o', label='Energy', color='C0')\n",
    "ax.plot(test_r, test_E, color='C0')\n",
    "ax.plot(test_r, test_pred, '-', label='Predicted Energy', color='C1')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0f20bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercises: \n",
    "\n",
    "1. Try varying the importance of the force and the energy in the loss function. Choose values such that you can convince yourself that both parts work. \n",
    "2. Try changing the size of the embedding. Whats the limit at which the network becomes unable to fit the potential?\n",
    "3. Make sense of the shape and size of the parameters of the model. Which input parameters influence them? "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "34e58ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In the original SchNet paper there's a nice figure showing the filters learned on a specific dataset. Fix the below code to produce that kind of figure.\n",
    "\n",
    "The steps are as follows: \n",
    "1. Make a tensor of linearly spaced distances\n",
    "2. Compute the RBF expansion of this tensor. \n",
    "3. Compute the filter for each expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = torch.linspace(0, 5*1.25, 100).reshape(-1, 1)\n",
    "\n",
    "distance_embedding_block = model.distance_embedding\n",
    "filter_block = model.sch_blocks[0].filter\n",
    "\n",
    "distance_embedding = distance_embedding_block(distance)\n",
    "W = filter_block(distance_embedding).detach().numpy()\n",
    "\n",
    "azm = np.linspace(0, 2 * np.pi, 200)\n",
    "r, th = np.meshgrid(distance.detach().numpy(), azm)\n",
    "\n",
    "nrows = 4 # Assumes a embedding dimension of 32.\n",
    "ncols = 8\n",
    "sz = 1.5\n",
    "fig, axes = plt.subplots(nrows, ncols, subplot_kw=dict(projection='polar'), figsize=(ncols*sz, nrows*sz))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    z = np.tile(W[:, i], (r.shape[0], 1))\n",
    "    ax.pcolormesh(th, r, z, cmap='coolwarm')\n",
    "    ax.axis('off')\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
