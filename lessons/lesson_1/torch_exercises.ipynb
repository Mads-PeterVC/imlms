{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d33b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f32fd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "82d706ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pytorch is a package that is very commonly used for machine learning. One primary \n",
    "reason for this is *automatic differentiation*.\n",
    "\n",
    "As you probably know analytically differentiating a function is a tiresome process and \n",
    "implementing a program to calculate them is generally more complicated than calculating \n",
    "the function itself. \n",
    "\n",
    "Auto differentiation is a way of avoiding this hurdle. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2059e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Auto differentiation of a simple function."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4d0e813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can try auto-diff for the trivial function \n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$\n",
    "\n",
    "Where the derivative is \n",
    "\n",
    "$$\n",
    "f'(x) = 2x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def f_prime(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "28bb8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nothing special about these function definitions, they are completely normal Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c30a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 10, 100, requires_grad=True) # Line 1\n",
    "y = f(x) # Line 2\n",
    "y_prime_auto = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y))[0] # Line 3"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3da202af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here some magic happens that we will digest later, but first visually that the \n",
    "two ways of calculating the derivative worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "y_prime = f_prime(x) # Calculate the analytical derivative to compare against. \n",
    "\n",
    "ax.plot(x.detach().numpy(), y_prime.detach().numpy(), label=\"f'(x)\")\n",
    "ax.plot(x.detach().numpy(), y_prime_auto.detach().numpy(), '--', label=\"f'(x) auto\")\n",
    "\n",
    "difference = torch.abs(y_prime - y_prime_auto).sum()\n",
    "ax.set_title(f\"Sum of differences: {difference.item():.8f}\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c8cd959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now lets work through the details of the calculation\n",
    "\n",
    "1. `x = torch.linspace(0, 10, 100, requires_grad=True)`\n",
    "    - Here we create a `torch.Tensor` of values ranging from 1 to 100. The `requires_grad=True` argument tells torch that we will want gradients wrt. `x` of any calculations done with `x`. \n",
    "2. `y = f(x)`\n",
    "    - We calculate the function value, nothing special here except that because `x` is a `torch.Tensor` the calculation happens in torch. \n",
    "3. `y_prime_auto = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y))[0]`\n",
    "    - The `torch.autograd.grad` function will compute the derivative of the first argument wrt. to the second. The `grad_outputs=torch.ones_like(y)` argument is only necessary because we have vectors for `y` rather than just a single scalar. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "43c61237",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Another polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "6cfaf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider the function\n",
    "\n",
    "$$\n",
    "f(x) = x^3 + x^2 + x\n",
    "$$\n",
    "Use torch to compute the derivative in the interval [0, 10] and compare to the analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    pass # Your code here\n",
    "\n",
    "def f_prime(x):\n",
    "    pass # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0 # Replace with your code\n",
    "y = 0 # Replace with your code\n",
    "y_prime_auto = 0 # Replace with your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "y_prime = f_prime(x) # Calculate the analytical derivative to compare against. \n",
    "\n",
    "ax.plot(x.detach().numpy(), y_prime.detach().numpy(), label=\"f'(x)\")\n",
    "ax.plot(x.detach().numpy(), y_prime_auto.detach().numpy(), '--', label=\"f'(x) auto\")\n",
    "\n",
    "difference = torch.abs(y_prime - y_prime_auto).sum()\n",
    "ax.set_title(f\"Sum of differences: {difference.item():.8f}\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0407aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Higher-order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "69c7e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "We are not limited to just first-order derivatives, we can compute derivatives to arbitrary order using \n",
    "autodifferentiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ede0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3547e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 10, 100, requires_grad=True)\n",
    "y = f(x)\n",
    "y_prime_auto = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "y_prime_prime_auto = torch.autograd.grad(y_prime_auto, x, grad_outputs=torch.ones_like(y_prime_auto))[0]\n",
    "\n",
    "print(y_prime_prime_auto) # This will print a tensor of 2's as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0e8426af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Multi-dimensional function"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f15adb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can use autodifferentiation to get all the partial derivatives of a function of \n",
    "multiple variables, e.g\n",
    "\n",
    "$$\n",
    "f(x, y) = 3 x^2 + 7 y^2 + xy\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81debcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(tensor):\n",
    "    x = tensor[0]\n",
    "    y = tensor[1]\n",
    "    return 3 * x**2 + 7 * y**2 + x * y\n",
    "\n",
    "xy = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "z = f(xy)\n",
    "df = torch.autograd.grad(z, xy, create_graph=True)[0]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "05ae305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Electric Field"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "96599fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider the scalar potential of an electric charge \n",
    "\n",
    "$$\n",
    "V(\\bar{r}) = \\frac{kq}{|\\bar{r}-\\bar{r_0}|}\n",
    "$$\n",
    "\n",
    "Where $r_0$ is the position of the charge. In two dimensions we could write this as\n",
    "\n",
    "$$\n",
    "V(x, y) = \\frac{kq}{\\sqrt{(x-x_0)^2 + (y-y_0)^2}}\n",
    "$$\n",
    "\n",
    "As we know, the electric field is the negative gradient of the potential \n",
    "$$\n",
    "\\mathbf{E} = -\\nabla V\n",
    "$$\n",
    "Use Pytorch to calculate the electric field.\n",
    "\n",
    "Start by defining a function that calculates the potential as a function of a tensor $X$\n",
    "where the first element is $x$ and the second is $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0334b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 5.0\n",
    "y0 = 5.0\n",
    "\n",
    "def V(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fa4bc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now define another function that calculates the electric field using automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d95e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(X):\n",
    "    # Your code here\n",
    "    e = 0 \n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9152f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "Then we can try the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9215017",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "e = E(X)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "05d93560",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can plot the field using a quiver plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b517732",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "field = torch.zeros((N, N, 2))\n",
    "\n",
    "x = torch.linspace(0, 10, N)\n",
    "y = torch.linspace(0, 10, N)\n",
    "\n",
    "for i, _x in enumerate(x):\n",
    "    for j, _y in enumerate(y):\n",
    "\n",
    "        if (_x - x0)**2 + (_y - y0)**2 < 1.0: # This is just a hack to avoid the singularity at (x0, y0).\n",
    "            continue\n",
    "\n",
    "        X_tensor = torch.tensor([_x, _y], requires_grad=True)\n",
    "        field[i, j] = E(X_tensor)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "X, Y = torch.meshgrid(torch.linspace(0, 10, N), torch.linspace(0, 10, N))\n",
    "\n",
    "ax.quiver(X, Y, field[:, :, 0].detach().numpy(), \n",
    "          field[:, :, 1].detach().numpy(), \n",
    "          )\n",
    "\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "792bfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Which matches the expected field rather well. \n",
    "\n",
    "An extension of this exercise would be to have the potential be the result of multiple \n",
    "point-charges at different locations - so try that at some point if you like. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9e911824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Gradient Descent & Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c3e29e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "We have seen a few examples of gradients so far, another is gradient descent. \n",
    "With gradient descent we optimize a function by taking small steps in the direction \n",
    "of the gradient.\n",
    "\n",
    "There are many use cases of such optimizations, we can focus on one involving what one \n",
    "may call 'fitting', 'regression' or 'learning'. Lets say we have some experimental observations \n",
    "$[(x_1, y_1), .., (x_N, y_N)]$ that we wish to explain with a model $M_\\theta$ where $\\theta$ denotes the parameters of the model\n",
    "\n",
    "We want to find the parameters of our model that makes it fit the data the best, \n",
    "and to do so we want a function that we can minimize wrt. the parameters. One \n",
    "intuitive choice is \n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{N}\\sum_i^N \\Big(M_\\theta(x_i) - y_i\\Big)^2,\n",
    "$$\n",
    "\n",
    "which is the *mean squared error loss* between the model and the observations. \n",
    "\n",
    "We are free to choose our model, for this example we will choose a linear model\n",
    "\n",
    "$$\n",
    "M_\\theta(x) = \\alpha x + \\beta\n",
    "$$\n",
    "\n",
    "So our model has two parameters thus $\\theta = [\\alpha, \\beta]$. \n",
    "We want to minimize the loss $L$ wrt. the parameters $\\theta$ by iteratively \n",
    "taking small steps in the direction of the gradient of $L$ wrt. $\\theta$. \n",
    "That is, \n",
    "\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\alpha \\nabla_\\theta L\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is typically called the learning rate. We want to calculate $ \\nabla_\\theta L$ using automatic differentiation.\n",
    "\n",
    "Lets set this up, first we could define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb28ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, theta):\n",
    "    return theta[0] * x + theta[1]\n",
    "\n",
    "def loss_fn(y_pred, y):\n",
    "    return torch.mean((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a43eda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "The next cell creates the data we will fit to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.torch import get_example_data\n",
    "\n",
    "X, y = get_example_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fa72843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "And finally we are ready to implement the gradient descent loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor([1.0, 1.0], requires_grad=True) # Initial parameters for the model.\n",
    "\n",
    "y_pred_initial = model(X, theta)\n",
    "\n",
    "N_steps = 100\n",
    "learning_rate = 0.05\n",
    "\n",
    "for i in range(N_steps):\n",
    "\n",
    "    y_pred = model(X, theta)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Loss {i}: {loss.item():0.3f}\")\n",
    "\n",
    "    gradient = torch.autograd.grad(loss, theta)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        theta -= learning_rate * gradient\n",
    "\n",
    "y_pred_final = model(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax.plot(X.detach().numpy(), y_pred_initial.detach().numpy(), label=\"Initial\")\n",
    "    ax.plot(X.detach().numpy(), y_pred_final.detach().numpy(), label=\"Final\")\n",
    "    ax.scatter(X.detach().numpy(), y.detach().numpy(), label=\"Data\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "126a6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Damped Harmonic Oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "006f07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "After countless hours in the lab you have gathered data on the very important \n",
    "phenonenom of quark anti-space mega amplitude. You believe that the property \n",
    "you have measured should behave like a damped harmonic oscillator following the \n",
    "expression \n",
    "\n",
    "$$\n",
    "M(t) = A \\exp^{-\\gamma t} \\cos(\\omega t)\n",
    "$$\n",
    "\n",
    "Where $t$ is the time and $A$, $\\gamma$ and $\\omega$ are parameters that you wish to \n",
    "find.\n",
    "\n",
    "Use the same methodology as above to find the best set of parameters $\\theta = [A, \\gamma, \\omega]$. \n",
    "\n",
    "The cell below generates your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imlms.torch import get_damped_osc_dataset\n",
    "\n",
    "t, y = get_damped_osc_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c5b1d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Define your model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t, theta):\n",
    "    A, gamma, omega = theta\n",
    "    return A * torch.exp(-gamma * t) * torch.cos(omega * t)\n",
    "\n",
    "def loss_fn(y_pred, y):\n",
    "    return torch.mean((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "37d8c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Then create the training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor([1.0, 1.0, 1.0], requires_grad=True) # Initial parameters for the model.\n",
    "\n",
    "y_pred_initial = model(t, theta)\n",
    "\n",
    "N_steps = 200 # You can try other values\n",
    "learning_rate = 0.05 # You can try other values\n",
    "\n",
    "for i in range(N_steps):\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "    with torch.no_grad():\n",
    "        theta -= 0 # Your code here\n",
    "\n",
    "y_pred_final = model(t, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ba814",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax.plot(t.detach().numpy(), y_pred_initial.detach().numpy(), label=\"Initial\")\n",
    "    ax.plot(t.detach().numpy(), y_pred_final.detach().numpy(), label=\"Final\")\n",
    "    ax.scatter(t.detach().numpy(), y.detach().numpy(), label=\"Data\")\n",
    "\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
